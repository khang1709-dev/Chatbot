import os
import pandas as pd
import torch
from tqdm import tqdm

# Load d·ªØ li·ªáu
from langchain_community.document_loaders import DataFrameLoader
from langchain_core.documents import Document

# Vector Store (D√πng g√≥i m·ªõi langchain_chroma)
from langchain_chroma import Chroma

# Embedding (D√πng g√≥i m·ªõi langchain_huggingface)
from langchain_huggingface import HuggingFaceEmbeddings

# (T√πy ch·ªçn) N·∫øu sau n√†y c·∫ßn chia nh·ªè vƒÉn b·∫£n th√¨ d√πng c√°i n√†y:
from langchain_text_splitters import RecursiveCharacterTextSplitter

# --- C·∫§U H√åNH ---
INPUT_FILE = "C:\\Users\\HP\\Downloads\\NLP API\\Chia_chunks_Full_Finalll.xlsx"
# ƒê·ªïi t√™n folder DB ƒë·ªÉ tr√°nh nh·∫ßm v·ªõi c√°i c≈©
PERSIST_DIRECTORY = "chroma_db_bge_m3"

def main():
    # 1. KI·ªÇM TRA GPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üöÄ THI·∫æT B·ªä: {device.upper()}")
    if device == "cpu":
        print("‚ö†Ô∏è C·∫¢NH B√ÅO: N√™n d√πng GPU T4 tr√™n Colab ƒë·ªÉ ch·∫°y BGE-M3.")

    # 2. ƒê·ªåC FILE
    print(f"\n--- ƒêang ƒë·ªçc file: {INPUT_FILE} ---")
    if not os.path.exists(INPUT_FILE):
        print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file.")
        return

    try:
        # ƒê·ªçc file (engine openpyxl cho xlsx)
        df = pd.read_excel(INPUT_FILE, engine='openpyxl')
        print(f"-> ƒê√£ ƒë·ªçc {len(df)} d√≤ng d·ªØ li·ªáu.")
    except Exception as e:
        print(f"L·ªói ƒë·ªçc file: {e}")
        return

    # 3. CHU·∫®N B·ªä DOCUMENTS (T·ªêI ∆ØU CHO BGE-M3)
    documents = []
    print("--- ƒêang chu·∫©n b·ªã Documents ---")

    # BGE-M3 kh√¥ng c·∫ßn th√™m "passage: " nh∆∞ E5
    # Vi·ªác n√†y gi√∫p gi·ªØ nguy√™n vƒÉn b·∫£n g·ªëc v√† ti·∫øt ki·ªám token

    for index, row in df.iterrows():
        clean_content = str(row['page_content']).strip()

        # Metadata gi·ªØ nguy√™n
        metadata = {
            "symbol": str(row['symbol']).upper().strip() if pd.notna(row['symbol']) else "UNKNOWN",
            "year": int(row['year']) if pd.notna(row['year']) else 0,
            "report_type": str(row['report_type']) if pd.notna(row['report_type']) else "UNKNOWN",
            "source": str(row['source']) if pd.notna(row['source']) else "UNKNOWN"
        }
        documents.append(Document(page_content=clean_content, metadata=metadata))

    # 4. LOAD MODEL BGE-M3 (SOTA HI·ªÜN T·∫†I)
    print("\n--- ƒêang t·∫£i Model BAAI/bge-m3 (M·∫°nh h∆°n E5)... ---")

    model_kwargs = {"device": device}
    encode_kwargs = {"normalize_embeddings": True} # BGE khuy·∫øn ngh·ªã normalize

    embedding_model = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )

    # 5. VECTOR H√ìA V√Ä L∆ØU
    print(f"\n--- B·∫Øt ƒë·∫ßu Vector h√≥a v√†o '{PERSIST_DIRECTORY}' ---")

    vector_db = Chroma(
        embedding_function=embedding_model,
        collection_name="financial_reports_bge",
        persist_directory=PERSIST_DIRECTORY
    )

    # Gi·∫£m batch size xu·ªëng 32 ho·∫∑c 16 v√¨ BGE-M3 n·∫∑ng h∆°n E5-base
    batch_size = 32
    total_docs = len(documents)

    for i in tqdm(range(0, total_docs, batch_size), desc="ƒêang Vector h√≥a"):
        batch = documents[i : i + batch_size]
        vector_db.add_documents(batch)

    print("\n==========================================")
    print("üéâ HO√ÄN T·∫§T! ƒê√£ n√¢ng c·∫•p l√™n model BGE-M3.")
    print(f"L∆∞u t·∫°i: {PERSIST_DIRECTORY}")
    print("==========================================")

if __name__ == "__main__":
    main()